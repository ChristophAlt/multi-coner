{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a9568b-274e-4b5e-9e29-b4a8eb47165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b70389e-2ac4-41a5-bb6c-aa4ecb1d5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from pytorch_ie import Document, Pipeline\n",
    "from src.models.span_clf_with_gazetteer import SpanClassificationWithGazetteerModel\n",
    "from src.taskmodules.span_clf_with_gazetteer import SpanClassificationWithGazetteerTaskModule\n",
    "from src.datamodules.datasets.multiconer import load_multiconer\n",
    "\n",
    "from eval_multiconer import seqeval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd57c935-3d8b-4a58-ae65-29122591de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_models(checkpoint1, checkpoint2):\n",
    "    model1 = SpanClassificationWithGazetteerModel.load_from_checkpoint(checkpoint1)\n",
    "    model2 = SpanClassificationWithGazetteerModel.load_from_checkpoint(checkpoint2)\n",
    "    \n",
    "    beta = 0.5 #The interpolation parameter    \n",
    "    params1 = model1.named_parameters()\n",
    "    params2 = model2.named_parameters()\n",
    "\n",
    "    dict_params2 = dict(params2)\n",
    "\n",
    "    for name1, param1 in params1:\n",
    "        # if name1 in dict_params2:\n",
    "        dict_params2[name1].data.copy_(beta*param1.data + (1-beta)*dict_params2[name1].data)\n",
    "\n",
    "    model1.load_state_dict(dict_params2, strict=False)\n",
    "    \n",
    "    return model1\n",
    "    \n",
    "    \n",
    "#     assert len(model_checkpoints) >= 1\n",
    "    \n",
    "#     models = [SpanClassificationWithGazetteerModel.load_from_checkpoint(checkpoint) for checkpoint in model_checkpoints]\n",
    "    \n",
    "#     num_models = len(models)\n",
    "    \n",
    "#     model1 = models[0]\n",
    "    \n",
    "#     with torch.inference_mode():\n",
    "#         for name1, param1 in model1.named_parameters():\n",
    "#             for i in range(1, num_models):\n",
    "#                 if name1 in models[i].named_parameters():\n",
    "#                     param_i = models[i].named_parameters()[name1]\n",
    "#                     param1.data.add_(param_i.data)\n",
    "\n",
    "#         for name1, param1 in model1.named_parameters():\n",
    "#             param1.div_(num_models)\n",
    "    \n",
    "#     return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fab78cb-e7de-4ea3-a124-53d9602d38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/media/christoph/HDD/models/multiconer-en-spanclf-wiki-gazetteer-freeze_3/\"\n",
    "DATASET_DIR = \"/home/christoph/Downloads/public_data/\"\n",
    "\n",
    "DATASET_SPLIT = \"validation\"\n",
    "FILTER_ENTITIES = False\n",
    "\n",
    "CHECKPOINT = \"multiconer-en-15-val_f1-0.91.ckpt\"\n",
    "\n",
    "CUDA_DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf312d2-b682-4b3c-9e59-ef33ec806d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "taskmodule = SpanClassificationWithGazetteerTaskModule.from_pretrained(MODEL_PATH, use_efficient_gazetteer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf4d274-f5e8-4728-8bc1-9e2076752844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = average_models(\n",
    "    # \"/media/christoph/HDD/models/multiconer-en-spanclf-wiki-gazetteer-freeze_1/multiconer-en-06-val_f1-0.89.ckpt\",\n",
    "    \"/media/christoph/HDD/models/multiconer-en-spanclf-wiki-gazetteer-freeze_2/multiconer-en-08-val_f1-0.89.ckpt\",\n",
    "    \"/media/christoph/HDD/models/multiconer-en-spanclf-wiki-gazetteer-freeze_3/multiconer-en-09-val_f1-0.89.ckpt\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(model=model, taskmodule=taskmodule, device=CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddacec57-7b85-4ed8-8779-7f2a65b79de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-b2b539f3793511b0\n",
      "Reusing dataset multi_co_ner (/home/christoph/.cache/huggingface/datasets/multi_co_ner/en-b2b539f3793511b0/1.0.0/afa61df806aafde79b4bd38aef1a3db19216190e1aa77a223a2d70d1eea327c9)\n",
      "100%|██████████| 800/800 [00:00<00:00, 15988.96it/s]\n",
      "800it [00:00, 939.03it/s]\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CORP     0.9392    0.8808    0.9091       193\n",
      "          CW     0.8547    0.8352    0.8448       176\n",
      "         GRP     0.9072    0.9263    0.9167       190\n",
      "         LOC     0.8966    0.8889    0.8927       234\n",
      "         PER     0.9721    0.9621    0.9671       290\n",
      "        PROD     0.7911    0.8503    0.8197       147\n",
      "\n",
      "   micro avg     0.9028    0.8984    0.9006      1230\n",
      "   macro avg     0.8935    0.8906    0.8917      1230\n",
      "weighted avg     0.9041    0.8984    0.9009      1230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_documents = load_multiconer(\n",
    "    data_dir=DATASET_DIR,\n",
    "    name=\"en\",\n",
    "    split=DATASET_SPLIT,\n",
    ")\n",
    "\n",
    "predict_field = \"entities\"\n",
    "\n",
    "pipeline(eval_documents, predict_field=predict_field)\n",
    "\n",
    "print(seqeval_score(documents=eval_documents, predict_field=predict_field, filter_entities=FILTER_ENTITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "600fe7fe-1e24-44d7-b70a-0e22d851de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/media/christoph/HDD/models/multiconer-en-spanclf-wiki-gazetteer-freeze-avg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93af822e-6e6f-4191-a7ae-e19bbf301590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using custom data configuration en-b2b539f3793511b0\n",
      "Reusing dataset multi_co_ner (/home/christoph/.cache/huggingface/datasets/multi_co_ner/en-b2b539f3793511b0/1.0.0/afa61df806aafde79b4bd38aef1a3db19216190e1aa77a223a2d70d1eea327c9)\n",
      "100%|██████████| 800/800 [00:00<00:00, 15496.51it/s]\n",
      "800it [00:00, 874.57it/s]\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CORP     0.8854    0.8808    0.8831       193\n",
      "          CW     0.9045    0.8068    0.8529       176\n",
      "         GRP     0.9185    0.8895    0.9037       190\n",
      "         LOC     0.8979    0.9017    0.8998       234\n",
      "         PER     0.9790    0.9655    0.9722       290\n",
      "        PROD     0.8121    0.8231    0.8176       147\n",
      "\n",
      "   micro avg     0.9086    0.8886    0.8985      1230\n",
      "   macro avg     0.8996    0.8779    0.8882      1230\n",
      "weighted avg     0.9089    0.8886    0.8983      1230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = SpanClassificationWithGazetteerModel.load_from_checkpoint(os.path.join(MODEL_PATH, CHECKPOINT))\n",
    "pipeline2 = Pipeline(model=model2, taskmodule=taskmodule, device=CUDA_DEVICE)\n",
    "\n",
    "eval_documents = load_multiconer(\n",
    "    data_dir=DATASET_DIR,\n",
    "    name=\"en\",\n",
    "    split=DATASET_SPLIT,\n",
    ")\n",
    "\n",
    "predict_field = \"entities\"\n",
    "\n",
    "pipeline2(eval_documents, predict_field=predict_field)\n",
    "\n",
    "print(seqeval_score(documents=eval_documents, predict_field=predict_field, filter_entities=FILTER_ENTITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b89551-9fca-4e03-8ccd-2897f72254fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
