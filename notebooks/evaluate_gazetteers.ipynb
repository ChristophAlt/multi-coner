{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6d6332-d60b-4026-b17c-e7dda0b4d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9d021a-cb44-4a11-922d-8208bc7ecc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from src.taskmodules.span_clf_with_gazetteer import Gazetteer, EfficientGazetteer\n",
    "from pytorch_ie.data.document import LabeledSpan\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.datamodules.datasets.multiconer import load_multiconer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8d1d92-9ee1-4e8f-a86c-3137bdfa1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_documents(documents, annotation_field: str = \"entities\", show_annotations: bool = True, show_predictions: bool = True):\n",
    "    for i, document in enumerate(documents):\n",
    "        print(f\"[{i}]\" + \"=\" * 100)\n",
    "        print(document.text)\n",
    "\n",
    "        if show_annotations:\n",
    "            print(\"Annotations:\")\n",
    "            for entity in sorted(document.annotations(annotation_field), key=lambda ent: ent.start):\n",
    "                entity_text = document.text[entity.start : entity.end]\n",
    "                label = entity.label\n",
    "                print(f\"{entity_text} -> {label}\")\n",
    "\n",
    "        if show_predictions:\n",
    "            print(\"*\" * 25)\n",
    "            print(\"Predictions:\")\n",
    "            for entity in sorted(document.predictions(annotation_field), key=lambda ent: ent.start):\n",
    "                entity_text = document.text[entity.start : entity.end]\n",
    "                label = entity.label\n",
    "                print(f\"{entity_text} -> {label} [{entity.score}]\")\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e368d70f-9c23-45c3-83a3-09358ea94f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_gazetteer(documents, tokenizer, gazetteer, min_span_length=1, max_span_length=8, max_length=128, score: float = 1.0):\n",
    "    is_efficient_gazetteer = isinstance(gazetteer, EfficientGazetteer)\n",
    "    \n",
    "    for document in tqdm.tqdm(documents):\n",
    "        text = document.text\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            document.text,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            max_length=max_length,\n",
    "            is_split_into_words=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        seq_length = len(inputs[\"input_ids\"])\n",
    "        \n",
    "        for span_length in range(min_span_length, max_span_length + 1):\n",
    "            \n",
    "            for start_index in range(seq_length + 1 - span_length):\n",
    "                end_index = start_index + span_length\n",
    "                \n",
    "                if is_efficient_gazetteer:\n",
    "                    span_text = tuple(inputs[\"input_ids\"][start_index:end_index])\n",
    "                else:\n",
    "                    span_text = tokenizer.decode(inputs[\"input_ids\"][start_index:end_index])\n",
    "                \n",
    "                labels = gazetteer.lookup(span_text)\n",
    "\n",
    "                start = inputs[\"offset_mapping\"][start_index][0]\n",
    "                end = inputs[\"offset_mapping\"][end_index - 1][1]\n",
    "                \n",
    "                for label in labels:\n",
    "                    if label is None:\n",
    "                        continue\n",
    "                    document.add_prediction(\"entities\", LabeledSpan(start=start, end=end, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8735f7d-651a-41c9-85a5-04e4fa8bd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_from_documents(documents, annotation_field: str = \"entities\"):\n",
    "    labels = [\"CORP\", \"PROD\", \"GRP\", \"CW\", \"LOC\", \"PER\"]\n",
    "    label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for document in documents:\n",
    "\n",
    "        entity_annotations = document.annotations(annotation_field)\n",
    "        entity_predictions = document.predictions(annotation_field)\n",
    "\n",
    "        span_to_annotations = {}\n",
    "        for entity in entity_annotations:\n",
    "            span = (entity.start, entity.end)\n",
    "\n",
    "            if span not in span_to_annotations:\n",
    "                span_to_annotations[span] = set()\n",
    "\n",
    "            span_to_annotations[span].add(entity.label)\n",
    "\n",
    "        span_to_predictions = {}\n",
    "        for entity in entity_predictions:\n",
    "            span = (entity.start, entity.end)\n",
    "\n",
    "            if span not in span_to_predictions:\n",
    "                span_to_predictions[span] = set()\n",
    "\n",
    "            span_to_predictions[span].add(entity.label)\n",
    "\n",
    "        visited_spans = set()\n",
    "        for span, annotations in span_to_annotations.items():\n",
    "            visited_spans.add(span)\n",
    "            \n",
    "            y_t = [0] * len(label_to_id)\n",
    "            y_p = [0] * len(label_to_id)\n",
    "\n",
    "            for annotation in annotations:\n",
    "                y_t[label_to_id[annotation]] = 1\n",
    "\n",
    "            for prediction in span_to_predictions.get(span, []):\n",
    "                y_p[label_to_id[prediction]] = 1\n",
    "\n",
    "            y_true.append(y_t)\n",
    "            y_pred.append(y_p)\n",
    "        \n",
    "        for span, predictions in span_to_predictions.items():\n",
    "            if span in visited_spans:\n",
    "                continue\n",
    "            \n",
    "            visited_spans.add(span)\n",
    "            \n",
    "            y_t = [0] * len(label_to_id)\n",
    "            y_p = [0] * len(label_to_id)\n",
    "            \n",
    "            for prediction in predictions:\n",
    "                y_p[label_to_id[prediction]] = 1\n",
    "            \n",
    "            y_true.append(y_t)\n",
    "            y_pred.append(y_p)\n",
    "    \n",
    "    return classification_report(y_true, y_pred, target_names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20ea158-20eb-4832-bc48-aec48b19b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_CONER_DIR = \"/home/christoph/Downloads/public_data/\"\n",
    "MULTI_CONER_SPLIT = \"train\"\n",
    "\n",
    "TOKENIZER_NAME_OR_PATH = \"google/electra-large-discriminator\"\n",
    "\n",
    "GAZETTEERS_DIR = \"../data/gazetteers/\"\n",
    "WIKIDATA_ENTITY_ALIASES_PATH = \"/home/christoph/Downloads/wikidata5m_alias/wikidata5m_entity.txt\"\n",
    "WIKIDATA_GRAPH_PATH = \"/home/christoph/Downloads/wikidata5m_all_triplet.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d821ba07-6e77-4928-86d1-0a882849c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25a29a5-75ec-43c4-8e38-d3b451407446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-b2b539f3793511b0\n",
      "Reusing dataset multi_co_ner (/home/christoph/.cache/huggingface/datasets/multi_co_ner/en-b2b539f3793511b0/1.0.0/afa61df806aafde79b4bd38aef1a3db19216190e1aa77a223a2d70d1eea327c9)\n",
      "  0%|                                                                                                                                                                                      | 0/15300 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 989.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 990.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 1004.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 991.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 999.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 987.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 981.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 999.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 993.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15300/15300 [00:15<00:00, 999.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CORP       0.26      0.70      0.38      3111\n",
      "        PROD       0.13      0.01      0.02      2923\n",
      "         GRP       0.10      0.47      0.17      3571\n",
      "          CW       0.14      0.60      0.22      3752\n",
      "         LOC       0.22      0.46      0.30      4799\n",
      "         PER       0.49      0.91      0.64      5397\n",
      "\n",
      "   micro avg       0.21      0.57      0.31     23553\n",
      "   macro avg       0.22      0.53      0.29     23553\n",
      "weighted avg       0.24      0.57      0.32     23553\n",
      " samples avg       0.20      0.24      0.21     23553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/christoph/miniconda3/envs/multi_coner/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_docs = load_multiconer(\n",
    "    data_dir=MULTI_CONER_DIR,\n",
    "    name=\"en\",\n",
    "    split=MULTI_CONER_SPLIT,\n",
    ")\n",
    "\n",
    "gazetteers = [\n",
    "    \"eng-wikidata-CORP.txt\",\n",
    "    \"eng-hltcoe-CORP.txt\",\n",
    "    \n",
    "    \"eng-hltcoe-PROD.txt\",\n",
    "    \n",
    "    \"eng-wikidata-GRP.txt\",\n",
    "    \"eng-hltcoe-GRP.txt\",\n",
    "    \n",
    "    \"eng-wikidata-CW.txt\",\n",
    "    \n",
    "    \"eng-wikidata-LOC.txt\",\n",
    "    \"eng-hltcoe-LOC.txt\",\n",
    "    \n",
    "    \"eng-wikidata-PER.txt\",\n",
    "    \"eng-hltcoe-PER.txt\",\n",
    "]\n",
    "\n",
    "for gazetteer_name in gazetteers:\n",
    "    gazetteer = Gazetteer(path=os.path.join(GAZETTEERS_DIR, gazetteer_name), lowercase=True)\n",
    "    predict_with_gazetteer(eval_docs, tokenizer=tokenizer, gazetteer=gazetteer, min_span_length=2, max_span_length=8)\n",
    "\n",
    "print(classification_report_from_documents(eval_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d18977-b4da-4ebc-a959-266ae819aeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]====================================================================================================\n",
      "his playlist includes sonny sharrock , gza , country teasers and the notorious b.i.g.\n",
      "Annotations:\n",
      "sonny sharrock -> PER\n",
      "gza -> PER\n",
      "country teasers -> GRP\n",
      "the notorious b.i.g. -> PER\n",
      "*************************\n",
      "Predictions:\n",
      "sonny sharrock -> PER [1.0]\n",
      "gza -> LOC [1.0]\n",
      "gza -> PER [1.0]\n",
      "gza -> PER [1.0]\n",
      "the notorious -> PER [1.0]\n",
      "the notorious b.i.g. -> PER [1.0]\n",
      "the notorious -> PER [1.0]\n",
      "b. -> CW [1.0]\n",
      "b. -> PER [1.0]\n",
      "i.g. -> CW [1.0]\n",
      "\n",
      "\n",
      "[1]====================================================================================================\n",
      "it is a series of badminton tournaments , sanctioned by badminton world federation ( bwf ) since 2007 .\n",
      "Annotations:\n",
      "badminton world federation -> GRP\n",
      "*************************\n",
      "Predictions:\n",
      "badminton world federation -> GRP [1.0]\n",
      "world federation -> GRP [1.0]\n",
      "since 2007 -> CW [1.0]\n",
      "\n",
      "\n",
      "[2]====================================================================================================\n",
      "all songs written by m.o.d. , unless otherwise stated\n",
      "Annotations:\n",
      "m.o.d. -> GRP\n",
      "*************************\n",
      "Predictions:\n",
      "written by -> CW [1.0]\n",
      "m.o.d. -> GRP [1.0]\n",
      "m. -> PER [1.0]\n",
      "o. -> PER [1.0]\n",
      "\n",
      "\n",
      "[3]====================================================================================================\n",
      "he worked in a bookstore before becoming a journalist , first for le devoir , and then for cité libre , for which he later became the director .\n",
      "Annotations:\n",
      "bookstore -> GRP\n",
      "le devoir -> GRP\n",
      "cité libre -> CW\n",
      "*************************\n",
      "Predictions:\n",
      "le devoir -> CORP [1.0]\n",
      "le devoir -> CW [1.0]\n",
      "devoir -> CW [1.0]\n",
      "and then -> CW [1.0]\n",
      "cité libre -> CW [1.0]\n",
      "the director -> CW [1.0]\n",
      "\n",
      "\n",
      "[4]====================================================================================================\n",
      "kingdom hospital , lewiston from stephen king miniseries of the same name\n",
      "Annotations:\n",
      "kingdom hospital -> CW\n",
      "lewiston -> LOC\n",
      "stephen king -> PER\n",
      "*************************\n",
      "Predictions:\n",
      "kingdom hospital -> CW [1.0]\n",
      "lewiston -> GRP [1.0]\n",
      "stephen king -> PER [1.0]\n",
      "stephen king -> PER [1.0]\n",
      "the same -> GRP [1.0]\n",
      "same name -> CW [1.0]\n",
      "\n",
      "\n",
      "[5]====================================================================================================\n",
      "rinder did not speak on camera because he promised his first interview to the bbc .\n",
      "Annotations:\n",
      "bbc -> CORP\n",
      "*************************\n",
      "Predictions:\n",
      "on camera -> CW [1.0]\n",
      "the bbc -> CORP [1.0]\n",
      "the bbc -> CORP [1.0]\n",
      "the bbc -> GRP [1.0]\n",
      "\n",
      "\n",
      "[6]====================================================================================================\n",
      "indigenous outlaw musquito defied colonial law and led attacks on settlers .\n",
      "Annotations:\n",
      "musquito -> PER\n",
      "*************************\n",
      "Predictions:\n",
      "mus -> GRP [1.0]\n",
      "mus -> LOC [1.0]\n",
      "mus -> LOC [1.0]\n",
      "musquito -> PER [1.0]\n",
      "mus -> PER [1.0]\n",
      "\n",
      "\n",
      "[7]====================================================================================================\n",
      "a code generation usually converts some intermediate representation of source code into machine code .\n",
      "Annotations:\n",
      "code generation -> CW\n",
      "*************************\n",
      "Predictions:\n",
      "intermediate representation -> CW [1.0]\n",
      "source code -> CW [1.0]\n",
      "machine code -> CW [1.0]\n",
      "\n",
      "\n",
      "[8]====================================================================================================\n",
      "the main contractor was ssangyong engineering and construction .\n",
      "Annotations:\n",
      "ssangyong engineering and construction -> CORP\n",
      "*************************\n",
      "Predictions:\n",
      "the main -> LOC [1.0]\n",
      "main contractor -> CORP [1.0]\n",
      "main contractor -> GRP [1.0]\n",
      "ssangyong engineering and construction -> CORP [1.0]\n",
      "ssangyong -> CORP [1.0]\n",
      "ssangyong engineering and construction -> CORP [1.0]\n",
      "ssangyong engineering and construction -> GRP [1.0]\n",
      "\n",
      "\n",
      "[9]====================================================================================================\n",
      "in 2010 , the musical instrument maestro raghav sachar was the highlight of the event .\n",
      "Annotations:\n",
      "raghav sachar -> PER\n",
      "*************************\n",
      "Predictions:\n",
      "raghav -> PER [1.0]\n",
      "raghav sachar -> PER [1.0]\n",
      "the event -> CW [1.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_documents(eval_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06beb01a-7d80-4383-b363-2a4af7e8b76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
